---
## This is a defaults file, it's likely to be changed during repo updates
## To override, or update settings, or configuration, create a new file
## in this directory, e.g. "overrides.yml" which will take precedence.

# We need to set these to the defaults because they are used in some playbooks.
mon_group_name: mons
osd_group_name: osds
rgw_group_name: rgws
mgr_group_name: mgrs

# We need to mirror the ceph-ansible default for releases <-> number conversion.
# This allows this var to be used without having to call the ceph-defaults role.
ceph_release_num:
  dumpling: 0.67
  emperor: 0.72
  firefly: 0.80
  giant: 0.87
  hammer: 0.94
  infernalis: 9
  jewel: 10
  kraken: 11
  luminous: 12
  mimic: 13

ceph_origin: 'repository' # or 'distro' or 'local'
ceph_stable: true # use ceph stable branch
ceph_stable_release: luminous

# Journal size should be 2 * (expected throughput * filestore max sync interval)
# Our expected max throughput on non-SSD drives is about 140MB/s
# filestore max sync interval default is 5. So 14GB should be the journal size
# Let's round that up to 20GB as a lazy/safe option
journal_size: "{{ osd_journal_size | default('20480') }}"

osd_objectstore: filestore

radosgw_civetweb_num_threads: 8192
ceph_conf_overrides_all:
   global:
     debug_lockdep: "0/0"
     debug_context: "0/0"
     debug_crush: "0/0"
     debug_buffer: "0/0"
     debug_timer: "0/0"
     debug_filer: "0/0"
     debug_objecter: "0/0"
     debug_rados: "0/0"
     debug_rbd: "0/0"
     debug_journaler: "0/0"
     debug_objectcacher: "0/0"
     debug_client: "0/0"
     debug_osd: "0/0"
     debug_optracker: "0/0"
     debug_objclass: "0/0"
     debug_filestore: "0/0"
     debug_bluestore: "0/0"
     debug_journal: "0/0"
     debug_ms: "0/0"
     debug_tp: "0/0"
     debug_auth: "0/0"
     debug_finisher: "0/0"
     debug_heartbeatmap: "0/0"
     debug_perfcounter: "0/0"
     debug_asok: "0/0"
     debug_throttle: "0/0"
     debug_paxos: "0/0"
     debug_rgw: "0/0"
     debug_monc: "0/0"
     debug_mon: "0/0"
     max open files: 26234859
     osd_pool_default_pg_num: 32
     osd_pool_default_pgp_num: 32
     mon_osd_down_out_interval: 900
   client:
     rbd_cache_size: 134217728
     rbd_cache_max_dirty: 50331648
     rbd_cache_max_dirty_age: 15
     rbd_cache_target_dirty: 33554432
   osd:
     osd_snap_trim_priority: 1
     osd_snap_trim_sleep: 0.1
     osd_pg_max_concurrent_snap_trims: 1
     osd_scrub_sleep: 0.1
     osd_scrub_priority: 1
     osd_scrub_chunk_min: 1
     osd_scrub_chunk_max: 5
     osd_scrub_load_threshold: 10.0
     osd_scrub_min_interval: 129600
     osd_deep_scrub_interval: 1209600
     filestore_merge_threshold: 40
     filestore_split_multiple: 8
     filestore_max_sync_interval: 10

ceph_conf_overrides_extra: {}
# This allows us to have ceph_conf_overrides per group without overwriting the
# overall ceph_conf_overrides_all (above).
# Additionally, we can set extras as a variable override without having to
# respecify all the above settings.
ceph_conf_overrides: "{{ ceph_conf_overrides_all | combine((ceph_conf_overrides_mgr | default({})), (ceph_conf_overrides_rgw | default({})), (ceph_conf_overrides_mon | default({})), (ceph_conf_overrides_osd | default({})), ceph_conf_overrides_extra, recursive = True) }}"

os_tuning_params:
  - { name: kernel.pid_max, value: 4194303 }
  - { name: fs.file-max, value: 26234859 }
  - { name: vm.vfs_cache_pressure, value: 20 }
  - { name: vm.dirty_background_ratio, value: 3}
  - { name: vm.dirty_ratio, value: 10 }
  - { name: vm.swappiness, value: 0 }
  - { name: net.ipv4.tcp_slow_start_after_idle, value: 0 }
  - { name: net.ipv4.tcp_max_syn_backlog, value: 4096 }
  - { name: net.core.rmem_max, value: 56623104 }
  - { name: net.core.wmem_max, value: 56623104 }
  - { name: net.core.rmem_default, value: 56623104 }
  - { name: net.core.wmem_default, value: 56623104 }
  - { name: net.core.optmem_max, value: 40960 }
  - { name: net.ipv4.tcp_rmem, value: 4096 87380 56623104 }
  - { name: net.ipv4.tcp_wmem, value: 4096 87380 56623104 }
  - { name: net.netfilter.nf_conntrack_max, value: 1048576 }
  - { name: net.core.somaxconn, value: 8192 }

containerized_deployment: false

# rgw_bench defaults
bench_rgw_user_password:
bench_rgw_hummingbird_url:  https://github.com/troubling/hummingbird/releases/download/v1.0.0/hummingbird

### Set this password in user_vars
radosgw_keystone_admin_password:

radosgw_keystone: false
# We need to default this while until https://github.com/ceph/ceph-ansible/pull/2355 is merged
# Set to true if using PKI keys
radosgw_keystone_ssl: false

# Default this to ceph-ansible default so VIPs can be configured
radosgw_civetweb_port: 8080
radosgw_service_publicurl: "https://{{ external_lb_vip_address }}:{{ radosgw_civetweb_port }}/swift/v1"
radosgw_service_adminurl: "http://{{ internal_lb_vip_address }}:{{ radosgw_civetweb_port }}/swift/v1"
radosgw_service_internalurl: "http://{{ internal_lb_vip_address }}:{{ radosgw_civetweb_port }}/swift/v1"

## RadosGW VIP for HAProxy
external_lb_vip_address: 127.0.0.1
internal_lb_vip_address: 127.0.0.1
haproxy_default_services:
  - service:
      haproxy_service_name: ceph_rgw
      haproxy_backend_nodes: "{{ groups['rgws'] | default([]) }}"
      haproxy_port: "{{ radosgw_civetweb_port }}"
      haproxy_balance_type: "http"
      haproxy_backend_options:
        - "httpchk HEAD /"
  - service:
      haproxy_service_name: ceph_dashboard
      haproxy_backend_nodes: "{{ groups['mgrs'] | default([]) }}"
      haproxy_port: 7000
      haproxy_balance_type: "http"
      haproxy_backend_options:
        - "httpchk HEAD /"
      haproxy_backend_httpcheck_options:
        - "expect ! status 302"
