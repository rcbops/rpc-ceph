---
force_containers_destroy: True
force_containers_data_destroy: False
bootstrap_host_data_disk_device_force: False
ansible_addr_prefix: "172.29.236"
storage_addr_prefix: "172.29.244"
bridges:
  - name: "br-mgmt"
    ip_addr: "{{ ansible_addr_prefix }}.100"
  - name: "br-storage"
    ip_addr: "{{ storage_addr_prefix }}.100"
container_name: "{{ inventory_hostname }}"
physical_host: localhost
properties:
  service_name: "{{ inventory_hostname }}"
ansible_become: True
ansible_user: root
external_lb_vip_address: "{{ bootstrap_host_public_address | default(ansible_default_ipv4.address) }}"
internal_lb_vip_address: 172.29.236.100

## LXC container default bind mounts
lxc_container_default_bind_mounts:
  - host_directory: "/openstack/backup/{{ inventory_hostname }}"
    container_directory: "/var/backup"
  - host_directory: "/openstack/log/{{ inventory_hostname }}"
    container_directory: "/var/log"

lxc_container_wait_params:
  # Wait 3 seconds before attempting the first connection
  delay: 3
  # Wait 60 seconds for the container to respond
  timeout: 60

# LXC Settings
lxc_net_address: 10.255.255.1
lxc_net_dhcp_range: 10.255.255.2,10.255.255.253
lxc_net_netmask: 255.255.255.0
lxc_image_cache_server: rpc-repo.rackspace.com
lxc_container_backing_store: dir
lxc_net_bridge: lxcbr0
lxc_kernel_options:
  - { key: 'fs.inotify.max_user_instances', value: 1024 }

ansible_ssh_extra_args: >
  -o UserKnownHostsFile=/dev/null
  -o StrictHostKeyChecking=no
  -o UserKnownHostsFile=/dev/null
  -o StrictHostKeyChecking=no
  -o ServerAliveInterval=64
  -o ServerAliveCountMax=1024
  -o Compression=no
  -o TCPKeepAlive=yes
  -o VerifyHostKeyDNS=no
  -o ForwardX11=no
  -o ForwardAgent=yes
  -T

# Used to create the device mappings into the containers - not Ceph specific
device_mapping:
  - /dev/sda
  - /dev/sdb
  - /dev/sdc

# For CentOS lets not setup any repos or priorities
openstack_host_rdo_repos: []
pip_install_rdo_repos: []
pip_install_repo_priorities: []
openstack_host_repo_priorities: []

## Container specific Ceph vars for running services in containers
ceph_rgw_systemd_overrides:
  Service:
    PrivateDevices: False
ceph_mon_systemd_overrides:
  Service:
    PrivateDevices: False
ceph_osd_systemd_overrides:
  Service:
    PrivateDevices: False
ceph_mgr_systemd_overrides:
  Service:
    PrivateDevices: False

## Testing specific fio_bench vars
ebs_direct_bench: true
ebs_file_bench: true
fiobench_pgnum: 5
fiobench_pgpnum: 5
fio_direct_read_test_1024k_overrides:
  global:
    runtime: 10
fio_direct_write_test_1024k_overrides:
  global:
    runtime: 10
fio_direct_rw_mix_1024k_overrides:
  global:
    runtime: 10
fio_test_file_read_16k_overrides:
  global:
    runtime: 10
  fio_test_file:
    size: 10M
fio_test_file_write_16k_overrides:
  global:
    runtime: 10
  fio_test_file:
    size: 10M
fio_test_file_read_4k_overrides:
  global:
    runtime: 10
  fio_test_file:
    size: 10M
fio_test_file_write_4k_overrides:
  global:
    runtime: 10
  fio_test_file:
    size: 10M
fio_rbd_write_test_1024k_overrides:
  global:
    runtime: 10
fio_rbd_read_test_1024k_overrides:
  global:
    runtime: 10
fio_rbd_rw_mix_1024k_overrides:
  global:
    runtime: 10
fio_rbd_read_test_4k_overrides:
  global:
    runtime: 10
fio_rbd_write_test_4k_overrides:
  global:
    runtime: 10
fio_rbd_read_test_16k_overrides:
  global:
    runtime: 10
fio_rbd_write_test_16k_overrides:
  global:
    runtime: 10

### Testing valuses for rgw benchmark
bench_rgw_user_password: rgwbenchsecret
bench_rgw_hummingbird_url:  https://github.com/troubling/hummingbird/releases/download/v1.0.0/hummingbird
bench_rgw_concurrency:  15
bench_rgw_object_size: 256
bench_rgw_number_of_objects: 5000
bench_rgw_number_of_gets: 10000

### Testing specific Ceph vars
# Set the size of the OSD and journal devices
# NB if you change the size you will need to change the osd_journal_size
# We setup 3 osds + 1 journal per host, there are 3 hosts.
rpc_ceph_test_osd_size: 3G
# Small test cluster we should set the default pg_num & pgp_num to 32
ceph_conf_overrides_extra:
   global:
     osd_pool_default_pg_num: 32
     osd_pool_default_pgp_num: 32
# We don't have enough space in an AIO to run 20GB journal devices
journal_size: 1024
monitor_interface: eth1
repo_server_interface: eth1
public_network: 172.29.236.0/22
cluster_network: 172.29.244.0/22
osd_scenario: non-collocated
devices:
  - /dev/sda
  - /dev/sdb

dedicated_devices:
  - /dev/sdc
  - /dev/sdc

### MaaS settings
# Enable the API usage
maas_use_api: true
# Will restart the agent service ONLY once at the end of a site.yml run
maas_restart_independent: false
# Set the default notification plan, in the gate this is set here
maas_notification_plan: npTechnicalContactsEmail
# Set the MaaS entity to be ansible_hostname instead of inventory_hostname
maas_entity_name: "{{ ansible_hostname }}{{ maas_fqdn_extension }}"
# Allow ceph services to be inside containers.
maas_rpc_legacy_ceph: True
