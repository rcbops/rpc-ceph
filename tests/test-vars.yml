---
force_containers_destroy: True
force_containers_data_destroy: False
ansible_addr_prefix: "10.1.1"
storage_addr_prefix: "10.2.1"
maas_rpc_legacy_ceph: True
container_name: "{{ inventory_hostname }}"
physical_host: localhost
properties:
  service_name: "{{ inventory_hostname }}"
ansible_become: True
ansible_user: root
bridges:
  - name: "br-mgmt"
    ip_addr: "{{ ansible_addr_prefix }}.1"
  - name: "br-storage"
    ip_addr: "{{ storage_addr_prefix }}.1"


## LXC container default bind mounts
lxc_container_default_bind_mounts:
  - host_directory: "/openstack/backup/{{ inventory_hostname }}"
    container_directory: "/var/backup"
  - host_directory: "/openstack/log/{{ inventory_hostname }}"
    container_directory: "/var/log"

lxc_container_wait_params:
  # Wait 3 seconds before attempting the first connection
  delay: 3
  # Wait 60 seconds for the container to respond
  timeout: 60

# LXC Settings
lxc_net_address: 10.100.100.1
lxc_net_netmask: 255.255.255.0
lxc_net_dhcp_range: 10.100.100.2,10.100.100.99
lxc_net_bridge: lxcbr0
lxc_kernel_options:
  - { key: 'fs.inotify.max_user_instances', value: 1024 }

ansible_ssh_extra_args: >
  -o UserKnownHostsFile=/dev/null
  -o StrictHostKeyChecking=no
  -o UserKnownHostsFile=/dev/null
  -o StrictHostKeyChecking=no
  -o ServerAliveInterval=64
  -o ServerAliveCountMax=1024
  -o Compression=no
  -o TCPKeepAlive=yes
  -o VerifyHostKeyDNS=no
  -o ForwardX11=no
  -o ForwardAgent=yes
  -T

# Used to create the device mappings into the containers - not Ceph specific
device_mapping:
  - /dev/sda
  - /dev/sdb
  - /dev/sdc
  - /dev/sdd

# For CentOS lets not setup any repos or priorities
openstack_host_rdo_repos: []
pip_install_rdo_repos: []
pip_install_repo_priorities: []
uca_enable: False
openstack_host_repo_priorities: []

## Container specific Ceph vars for running services in containers
ceph_rgw_systemd_overrides:
  Service:
    PrivateDevices: False
ceph_mon_systemd_overrides:
  Service:
    PrivateDevices: False
ceph_osd_systemd_overrides:
  Service:
    PrivateDevices: False
ceph_mgr_systemd_overrides:
  Service:
    PrivateDevices: False

## Testing specific fio_bench vars
fio_direct_read_test_overrides:
  global:
    runtime: 10
fio_direct_write_test_overrides:
  global:
    runtime: 10
fio_rw_mix_overrides:
  global:
    runtime: 10
fio_test_file_read_overrides:
  global:
    runtime: 10
  fio_test_file:
    size: 10M
fio_test_file_write_overrides:
  global:
    runtime: 10
  fio_test_file:
    size: 10M

## Testing specific Ceph vars
# Set the size of the OSD and journal devices
# NB if you change the size you will need to change the osd_journal_size
# We setup 3 osds + 1 journal per host, there are 3 hosts.
rpc_ceph_test_osd_size: 3G
# We don't have enough space in an AIO to run 20GB journal devices
# Testing testing
osd_journal_size: 1024
monitor_interface: eth1
public_network: 10.1.1.0/24
cluster_network: 10.2.1.0/24
osd_scenario: non-collocated
devices:
  - /dev/sda1
  - /dev/sdb1

dedicated_devices:
  - /dev/sdc1
  - /dev/sdc1
